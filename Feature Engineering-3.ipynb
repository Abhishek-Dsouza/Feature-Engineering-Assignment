{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9e6a93-4231-4688-84a9-a42f68ccdaef",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9671fb-0e3a-4ce9-b956-4d65f3770907",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features to a specific range. The purpose of Min-Max scaling is to transform the values of different features into a consistent range, typically between 0 and 1, or any other desired range.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original value of a feature\n",
    "min_value is the minimum value of that feature in the dataset\n",
    "max_value is the maximum value of that feature in the dataset\n",
    "scaled_value is the transformed value within the desired range\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset of students' exam scores, and the range of scores is from 60 to 100. We want to normalize these scores to a range between 0 and 1.\n",
    "\n",
    "Original scores: [70, 85, 90, 60, 95]\n",
    "\n",
    "To apply Min-Max scaling, we need to calculate the minimum and maximum values of the scores in the dataset. In this case, the minimum value is 60, and the maximum value is 95.\n",
    "\n",
    "Using the Min-Max scaling formula, we can rescale each score:\n",
    "\n",
    "Scaled scores = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "For the score 70:\n",
    "scaled_score = (70 - 60) / (95 - 60) = 10 / 35 ≈ 0.2857\n",
    "\n",
    "For the score 85:\n",
    "scaled_score = (85 - 60) / (95 - 60) = 25 / 35 ≈ 0.7143\n",
    "\n",
    "For the score 90:\n",
    "scaled_score = (90 - 60) / (95 - 60) = 30 / 35 ≈ 0.8571\n",
    "\n",
    "For the score 60:\n",
    "scaled_score = (60 - 60) / (95 - 60) = 0 / 35 = 0\n",
    "\n",
    "For the score 95:\n",
    "scaled_score = (95 - 60) / (95 - 60) = 35 / 35 = 1\n",
    "\n",
    "The resulting scaled scores are: [0.2857, 0.7143, 0.8571, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "100e8849-daa7-43b8-ae7e-54000e6e2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8267bd3b-8c2d-4fe7-bc01-b8d991411dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee081fc-3359-432e-8572-972c3f9668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f41988-b460-4be9-8e60-89d522c2ffe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-27 17:53:01</td>\n",
       "      <td>2019-03-27 18:00:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.16</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-10 01:23:59</td>\n",
       "      <td>2019-03-10 01:49:51</td>\n",
       "      <td>1</td>\n",
       "      <td>7.70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Hudson Sq</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-30 13:27:42</td>\n",
       "      <td>2019-03-30 13:37:14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Midtown East</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
       "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
       "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
       "2 2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5  2.36   \n",
       "3 2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0  6.15   \n",
       "4 2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0  1.10   \n",
       "\n",
       "   tolls  total   color      payment            pickup_zone  \\\n",
       "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
       "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
       "2    0.0  14.16  yellow  credit card          Alphabet City   \n",
       "3    0.0  36.95  yellow  credit card              Hudson Sq   \n",
       "4    0.0  13.40  yellow  credit card           Midtown East   \n",
       "\n",
       "            dropoff_zone pickup_borough dropoff_borough  \n",
       "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1  Upper West Side South      Manhattan       Manhattan  \n",
       "2           West Village      Manhattan       Manhattan  \n",
       "3         Yorkville West      Manhattan       Manhattan  \n",
       "4         Yorkville West      Manhattan       Manhattan  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c44736-5c7b-4af7-928f-77fbed434dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pickup', 'dropoff', 'passengers', 'distance', 'fare', 'tip', 'tolls',\n",
       "       'total', 'color', 'payment', 'pickup_zone', 'dropoff_zone',\n",
       "       'pickup_borough', 'dropoff_borough'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e3b94c-708c-46ec-a0b7-d8f1b4a0ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit(df[['distance', 'fare', 'tip']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b98042-0716-4b01-a231-eb1d0b991056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(min_max.transform(df[['distance', 'fare', 'tip']]),columns = ['Distance','Fare','Tips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97aab72a-486d-49a9-8402-5652f8207bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.064759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.043624</td>\n",
       "      <td>0.071084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.174497</td>\n",
       "      <td>0.185241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058856</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Distance      Fare      Tips\n",
       "0  0.043597  0.040268  0.064759\n",
       "1  0.021526  0.026846  0.000000\n",
       "2  0.037330  0.043624  0.071084\n",
       "3  0.209809  0.174497  0.185241\n",
       "4  0.058856  0.053691  0.033133"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631ef2b-d8b8-4836-b293-25d75ec6354e",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c0320-1c74-4b37-a377-b964c64a1a14",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or feature scaling, is a data preprocessing technique that rescales the feature vectors to have a unit norm. In other words, it scales the vector to a length of 1 while preserving its direction.\n",
    "\n",
    "The purpose of the Unit Vector technique is to ensure that all feature vectors have the same scale, regardless of their magnitudes. This can be particularly useful in situations where the magnitude of the feature vectors is not as important as their direction.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "scaled_vector = vector / ||vector||\n",
    "\n",
    "where:\n",
    "\n",
    "vector is the original feature vector\n",
    "||vector|| represents the Euclidean norm (also known as L2 norm) of the vector, which is calculated as the square root of the sum of squared values of the vector's elements\n",
    "scaled_vector is the transformed feature vector with a unit norm\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset of two-dimensional feature vectors:\n",
    "\n",
    "Original feature vectors: [[3, 4], [1, 2], [6, 8], [2, 3]]\n",
    "\n",
    "To apply Unit Vector scaling, we need to calculate the Euclidean norm of each vector and then divide each vector by its norm.\n",
    "\n",
    "For the first vector [3, 4]:\n",
    "||vector|| = √(3^2 + 4^2) = √(9 + 16) = √25 = 5\n",
    "scaled_vector = [3, 4] / 5 = [0.6, 0.8]\n",
    "\n",
    "For the second vector [1, 2]:\n",
    "||vector|| = √(1^2 + 2^2) = √(1 + 4) = √5\n",
    "scaled_vector = [1, 2] / √5 ≈ [0.4472, 0.8944]\n",
    "\n",
    "For the third vector [6, 8]:\n",
    "||vector|| = √(6^2 + 8^2) = √(36 + 64) = √100 = 10\n",
    "scaled_vector = [6, 8] / 10 = [0.6, 0.8]\n",
    "\n",
    "For the fourth vector [2, 3]:\n",
    "||vector|| = √(2^2 + 3^2) = √(4 + 9) = √13\n",
    "scaled_vector = [2, 3] / √13 ≈ [0.5547, 0.8321]\n",
    "\n",
    "The resulting scaled feature vectors are: [[0.6, 0.8], [0.4472, 0.8944], [0.6, 0.8], [0.5547, 0.8321]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129ce14-5abe-41bd-8623-b13855dcd8ff",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c71931-194f-4ffa-bdce-7bfd02587661",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It is commonly applied to high-dimensional datasets to transform them into a lower-dimensional space while preserving the most important information.\n",
    "\n",
    "PCA works by identifying the principal components, which are new orthogonal axes in the data space that capture the maximum variance in the data. These principal components are ranked in order of importance, with the first principal component capturing the highest variance, the second capturing the second highest variance, and so on.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data: If the features in the dataset have different scales, it is necessary to standardize them to have zero mean and unit variance. This step ensures that all features contribute equally to the analysis.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix for the standardized data, which represents the relationships between different features.\n",
    "\n",
    "Perform eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, while the eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "Select the desired number of principal components: Determine the number of principal components to retain based on the explained variance. Generally, one may choose to retain components that explain a significant portion of the total variance, such as 90% or 95%.\n",
    "\n",
    "Project the data: Transform the original data onto the new lower-dimensional space defined by the selected principal components. This projection reduces the dimensionality while preserving the most important information.\n",
    "\n",
    "Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset with three features: height, weight, and age. We want to reduce the dimensionality of the dataset to two dimensions.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Data point 1: [170 cm, 65 kg, 25 years]\n",
    "Data point 2: [160 cm, 55 kg, 30 years]\n",
    "Data point 3: [180 cm, 70 kg, 28 years]\n",
    "Data point 4: [165 cm, 60 kg, 35 years]\n",
    "Standardize the data: Standardize the dataset by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "Perform eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. Let's assume we obtain two eigenvectors: [0.5, 0.8, 0.3] and [-0.2, 0.4, -0.9], with corresponding eigenvalues of 2.5 and 0.8.\n",
    "\n",
    "Select the desired number of principal components: Since we want to reduce the dimensionality to two, we select the two eigenvectors with the highest eigenvalues.\n",
    "\n",
    "Project the data: Transform the original data onto the new lower-dimensional space defined by the selected eigenvectors. We calculate the dot product of each data point with the two eigenvectors.\n",
    "\n",
    "Projected data:\n",
    "\n",
    "Projected data point 1: [0.5 * 170 + (-0.2) * 65, 0.8 * 170 + 0.4 * 65] = [78.5, 155]\n",
    "Projected data point 2: [0.5 * 160 + (-0.2) * 55, 0.8 * 160 + 0.4 * 55] = [72.5, 148]\n",
    "Projected data point 3: [0.5 * 180 + (-0.2) * 70, 0.8 * 180 + 0.4 * 70] ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82484a4-c8c1-43b9-a56c-c5f1758e76ab",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f87a6a-fc18-4141-98eb-e4b08609c2ba",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts, and PCA can be used as a technique for feature extraction.\n",
    "\n",
    "Feature extraction is the process of transforming the original set of features into a new set of features that captures the most important information while reducing dimensionality. The goal is to create a more concise representation of the data that retains relevant patterns and reduces noise or redundancy.\n",
    "\n",
    "PCA can be used for feature extraction by identifying the principal components, which are linear combinations of the original features. These principal components are derived from the covariance matrix of the data and capture the maximum variance in the dataset. By selecting a subset of the principal components, we effectively create a reduced feature space that represents the most important characteristics of the data.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of images represented by pixel intensities. Each image is 100 pixels by 100 pixels, resulting in 10,000 original features. We want to extract a smaller set of features that still captures the essential information of the images.\n",
    "\n",
    "Preprocess the data: Convert the images into a numerical representation, such as a matrix, where each row represents an image and each column represents a pixel intensity.\n",
    "\n",
    "Standardize the data: Standardize the pixel intensities to have zero mean and unit variance.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "Perform eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "Select the desired number of principal components: Determine the number of principal components to retain based on the explained variance. For example, we may choose to retain enough principal components that explain 90% or 95% of the total variance.\n",
    "\n",
    "Project the data: Transform the original data onto the new lower-dimensional space defined by the selected principal components. This projection reduces the dimensionality while preserving the most important information.\n",
    "\n",
    "By selecting a subset of principal components, we effectively reduce the dimensionality of the image dataset. The new set of features represents the most important patterns and variations in the images, allowing for more efficient computation and potentially improved performance in downstream tasks such as image classification or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e94b9-579c-4523-93e8-17de03ca51f6",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ac35b-b446-49b2-9023-89641c9ff659",
   "metadata": {},
   "source": [
    "To use Min-Max scaling to preprocess the data for building a recommendation system for a food delivery service, you would follow these steps:\n",
    "\n",
    "1. Identify the features: Determine which features from the dataset you want to include in your recommendation system. In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "2. Calculate the minimum and maximum values: Find the minimum and maximum values for each feature in the dataset. For example, for the price feature, find the minimum and maximum prices among all the food items in the dataset.\n",
    "\n",
    "3. Apply Min-Max scaling: Use the Min-Max scaling formula to transform the values of each feature into the desired range, typically between 0 and 1. The formula is:\n",
    "\n",
    "scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "For each data point, apply this formula to every feature individually.\n",
    "\n",
    "4. Repeat for each feature: Perform Min-Max scaling for each feature separately. Calculate the scaled values for rating and delivery time in the same manner as done for the price feature.\n",
    "\n",
    "5. Replace the original values: Replace the original values in the dataset with the scaled values obtained from Min-Max scaling. This will ensure that all features are now on a consistent scale within the desired range (0 to 1).\n",
    "\n",
    "The purpose of using Min-Max scaling in this scenario is to bring all the features to a common scale, allowing them to contribute equally during the recommendation process. Since features like price, rating, and delivery time may have different value ranges, scaling them using Min-Max scaling will prevent any particular feature from dominating the recommendation algorithm due to its larger range.\n",
    "\n",
    "By applying Min-Max scaling, you will have transformed the original values of price, rating, and delivery time into a normalized range of 0 to 1, making them suitable for use in the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9815d-81d5-4611-a050-cdff0d823fcd",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d12da3-1191-47dd-9e6b-324805a70cc1",
   "metadata": {},
   "source": [
    "To use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset for predicting stock prices, you would follow these steps:\n",
    "\n",
    "1. Identify the features: Determine the set of features from the dataset that you want to include in your stock price prediction model. These features can include company financial data (e.g., revenue, earnings, debt) and market trends (e.g., stock indices, interest rates).\n",
    "\n",
    "2. Preprocess the data: Preprocess the dataset by standardizing the features to have zero mean and unit variance. This step is essential to ensure that features with different scales do not dominate the PCA analysis.\n",
    "\n",
    "3. Compute the covariance matrix: Calculate the covariance matrix based on the standardized feature dataset. The covariance matrix represents the relationships and correlations between different features.\n",
    "\n",
    "4. Perform PCA: Apply PCA to the covariance matrix. This involves finding the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "5. Select the desired number of principal components: Determine the number of principal components to retain based on the explained variance. You can choose a threshold, such as retaining enough principal components that explain a certain percentage of the total variance, such as 90% or 95%.\n",
    "\n",
    "6. Project the data: Transform the original feature dataset onto the new lower-dimensional space defined by the selected principal components. This projection reduces the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "7. Train the prediction model: Use the reduced-dimensional feature dataset obtained from PCA as input to your stock price prediction model. The reduced feature space may make the model more computationally efficient and can help mitigate issues associated with the curse of dimensionality.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, you aim to capture the most important patterns and variations in the data while discarding or compressing less important information. This reduction in dimensionality can help simplify the prediction model, improve computational efficiency, and potentially mitigate issues such as overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ecc61-74f2-4f2a-a393-71d535569300",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c49a74b9-3690-4025-81a8-075ff86c23ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_val) / (max_val - min_val) * 2 - 1\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14dbf30-d050-470b-a10c-f3a85ad6d26c",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b7706-6adf-413f-b1d2-5b0bf4bf838a",
   "metadata": {},
   "source": [
    "To determine the number of principal components to retain for feature extraction using PCA, additional information about the dataset and its characteristics is needed. Specifically, the size of the dataset and the desired level of explained variance are important factors in making this decision.\n",
    "\n",
    "Here are the general steps to guide the process:\n",
    "\n",
    "Standardize the data: Preprocess the dataset by standardizing the features, ensuring they have zero mean and unit variance. This step is necessary to bring all the features to a common scale before applying PCA.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix based on the standardized feature dataset. The covariance matrix represents the relationships and correlations between the features.\n",
    "\n",
    "Perform PCA: Apply PCA to the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "Analyze the explained variance: Examine the explained variance ratios associated with each principal component. The explained variance ratio indicates the proportion of the total variance in the dataset captured by each principal component. This information helps in determining the number of principal components to retain.\n",
    "\n",
    "Decide on the number of principal components: Choose the number of principal components to retain based on the desired level of explained variance. This decision can be based on a threshold, such as retaining enough principal components to explain a certain percentage of the total variance (e.g., 90%, 95%).\n",
    "\n",
    "It's important to note that the number of principal components retained should strike a balance between dimensionality reduction and the amount of information preserved. Retaining too few principal components may result in loss of important information, while retaining too many may not significantly reduce the dimensionality.\n",
    "\n",
    "Without specific information about the dataset, it's not possible to determine the exact number of principal components to retain. However, a common approach is to plot the cumulative explained variance ratio against the number of principal components and select the number of components that capture a significant portion of the variance (e.g., 90% or higher).\n",
    "\n",
    "By visually inspecting the plot, you can observe the point at which the explained variance levels off or starts to show diminishing returns. This can help you determine the number of principal components that provide a good trade-off between dimensionality reduction and retaining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab74ccf-f7b0-4d19-85db-b17954b1df04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
